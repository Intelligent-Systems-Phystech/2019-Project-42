\documentclass[12pt,twoside]{article}
\usepackage{jmlda}
%\NOREVIEWERNOTES
\title
    [Z-learning of linearly-solvable Markov Decision Processes] % Краткое название; не нужно, если полное название влезает в~колонтитул
    {Z-learning of linearly-solvable Markov Decision Processes}
\author
    [Безносиков А.Н.] % список авторов для колонтитула; не нужен, если основной список влезает в колонтитул
    {Безносиков А.Н., Максимов Ю.В., Чертков М., Стрижов В.В.} % основной список авторов, выводимый в оглавление
    [Aleksandr Beznosikov$^1$, Yury Maximov, Michael Chertkov, Vadim Strijov] 
\thanks
    {Supervisor:  Vadim Strijov
   Task author:  Michael Chertkov
   Consultant:  Yury Maximov}
\email
    {beznosikov.an@phystech.edu}
\organization
    {$^1$Moscow Institute of Physics and Technology}
\abstract
    {Considered methods for solving the problem of discrete Markov Decision Process. For certain class of MPDs which greatly simplify Reinforcement Learning. In this paper we adapt a  modification (Z - learning) to the case of Markov Decision Process discussed in the context of energy systems and solve the optimal control problem by incomplete data. Comparing with standard Q-learning, show that modification of algorithm gives faster and more reliable solution. 

\bigskip
\textbf{Keywords}: \emph {Markov Decision Process, Z - learning, Q-learning}.}

\begin{document}
\maketitle

\section{Introduction} 

In the area of power systems there is a huge demand on fast reinforcement learning algorithms, but there is still a lack of that. In this paper we solve the problem of optimal energy system consisting of a set of devices from \cite{Chertkov}.

The behavior of the system of devices in time is considered as a discrete Markov process. In general, this problem is not solved simply. But in \cite{Todorov} there are several ways to solve it optimally. Most methods require knowledge of what happens if a system "left alone" long enough. But in practice it often happens that this information is hidden from us.

In this paper it is proposed to use the Z-learning method (stochastic modification of Q-learning from \cite{Szepesvari,Allen}). Together with the solution of the main task of the MDP in parallel to restore unknown data on the behavior of the system. With this algorithm, a working model of system management will be built based only on limited samples of representative behavior. We compare the speed and quality of the two algorithms when solving the MDP, describing via transition probability matrix. Given initial state vector (probability of being in a state at time zero), we generate data for the time evolution of the state vector. 



\bibliographystyle{plain}
\bibliography{Beznosikov201942}

\end{document}
